```python

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sinq.patch_model import AutoSINQHFModel
from sinq.sinqlinear import BaseQuantizeConfig

# Use CPU for quantization and inference (no GPU required)
model_name = "Qwen/Qwen3-1.7B"
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    torch_dtype=torch.bfloat16,
    device_map="cpu"  # Use CPU instead of GPU
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Optimized quantization config for CPU and memory constraints
quant_cfg = BaseQuantizeConfig(
    nbits=4,            # quantization bit-width
    group_size=None,    # No grouping to avoid dimension issues
    tiling_mode="1D",   # tiling strategy
    method="sinq"       # quantization method ("asinq" for the calibrated version)
)

# Quantize on CPU
AutoSINQHFModel.quantize_model(
    model,
    tokenizer=tokenizer,
    quant_config=quant_cfg,
    compute_dtype=torch.float16,  # Use float16 for better CPU compatibility
    device="cpu"  # Use CPU for quantization
)

# --- Save to a folder ---
from sinq.patch_model import AutoSINQHFModel

save_dir = "qwen3-1.7b-sinq-4bit"  # any path
AutoSINQHFModel.save_quantized(model, save_dir, verbose=True) # model is an already sinq-quantized model

# --- Reload later (no base FP weights needed) ---
from sinq.patch_model import AutoSINQHFModel
import torch

qmodel = AutoSINQHFModel.from_quantized(
    save_dir,
    device="cpu",  # Load on CPU
    compute_dtype=torch.float16,  # Use float16 for CPU
)

# (optional) quick smoke test - CPU version
prompt = "Explain neural network quantization in one sentence."
inputs = tokenizer(prompt, return_tensors="pt").to("cpu")  # Move to CPU
with torch.inference_mode():
    out_ids = qmodel.generate(**inputs, max_new_tokens=32, do_sample=False)
print(tokenizer.decode(out_ids[0], skip_special_tokens=True))

```